{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Tensors and Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"In mathematics, a tensor is an arbitrarily complex geometric object that maps in a multi-linear manner geometric vectors, scalars, and other tensors to a resulting tensor.\"** -- Wikipedia\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another excerpt from the Wikipedia article on tensors:\n",
    "\n",
    "> *Although seemingly different, the various approaches to defining tensors describe the same geometric concept using different language and at different levels of abstraction.*\n",
    "> ### *As multidimensional arrays*\n",
    "> *A tensor may be represented as a (potentially multidimensional) array (although a multidimensional array is not necessarily a representation of a tensor, as discussed below with regard to holors).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this class, when we say **tensor**, we will just mean **a multidimensional array**. But the term **dimension** can get confusingly overloaded here. Let's see why.\n",
    "\n",
    "A 1-dimensional tensor is just a vector. In this class, we'll predominantly be using the ```torch``` package for manipulating tensors. In ```torch```, we create a vector as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import tensor\n",
    "v = tensor([4, 5, 6, 7, 8])\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, but then this ends up being described as a vector of dimension 5, or **a 1-dimensional tensor of dimension 5**. Which is super confusing. So typically, when refer to these separate concepts as the **order** and the **shape**. So this is an **order-1 tensor of shape (5)**. We can get these numbers from the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The order of the tensor is 1.\n",
      "The shape of the tensor is torch.Size([5]).\n"
     ]
    }
   ],
   "source": [
    "print('The order of the tensor is {}.'.format(v.dim()))\n",
    "print('The shape of the tensor is {}.'.format(v.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ```.shape``` is an attribute of the tensor, not a method call. You can get parts of vectors in a similar way you would for Python lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v[:3])\n",
    "print(v[2:])\n",
    "print(v[2:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** what's a simple way to extract the vector ```[4, 6, 8]```?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "v[::2] # returns tensor([4, 6, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important special case is converting an order-1 tensor into a scalar. In ```torch```, they recommend doing this using ```.item()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tensor([12])\n",
    "w.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that ```.item()``` will throw an exception if you apply it to a non-singleton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An order-2 tensor is a **matrix**. Here's how we would create a matrix with 4 rows and 3 columns using ```torch```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12],\n",
      "        [13, 14, 15]])\n",
      "The order of M is 2.\n",
      "The shape of M is torch.Size([4, 3]).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "M = tensor([[4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15]])\n",
    "print(M)\n",
    "print('The order of M is {}.'.format(M.dim()))\n",
    "print('The shape of M is {}.'.format(M.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we get submatrices? It's sort of like the syntax for getting parts of Python lists, though a bit weird to get used to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M) \n",
    "print(M[:2]) # first two rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M) \n",
    "print(M[2:4]) # last two rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M) \n",
    "print(M[0::2]) # every second row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M) \n",
    "print(M[:,:2]) # first two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M) \n",
    "print(M[:,2:3]) # last column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M) \n",
    "print(M[:,0::2]) # every second column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** How do you extract ```tensor([[7, 9], [10, 12]])```?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7,  9],\n",
      "        [10, 12]])\n"
     ]
    }
   ],
   "source": [
    "print(M[1:3,0:3:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do the usual elementwise operations on matrices, and transpose them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8, 10, 12],\n",
      "        [14, 16, 18],\n",
      "        [20, 22, 24],\n",
      "        [26, 28, 30]])\n",
      "tensor([[12, 15, 18],\n",
      "        [21, 24, 27],\n",
      "        [30, 33, 36],\n",
      "        [39, 42, 45]])\n",
      "tensor([[ 16,  25,  36],\n",
      "        [ 49,  64,  81],\n",
      "        [100, 121, 144],\n",
      "        [169, 196, 225]])\n",
      "tensor([[ 4,  7, 10, 13],\n",
      "        [ 5,  8, 11, 14],\n",
      "        [ 6,  9, 12, 15]])\n"
     ]
    }
   ],
   "source": [
    "print(M+M)\n",
    "print(3*M)\n",
    "print(M*M) # note: this is elementwise multiplication (sometimes called Hadamard product), not matrix multiplication!\n",
    "print(M.t())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do standard matrix multiplication with the ```.mm()``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M.mm(M.t()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue that is likely going to bite you again and again is the datatype of matrices, which tells you what the entries of the matrices are: usually either floating point (of some kind) or integer (of some kind). ```torch``` does its best to guess what you want, but it's not psychic. For the matrix ```M2```, it assumes that because all the initialized entries are integers, then we want a matrix with an integer datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "M2 = tensor([[4,5,6],[2,8,9],[1,7,3]])\n",
    "print(M2.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But maybe we wanted these to be floating point, and unsuspectingly try to take the matrix inverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.inverse(M2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially it's complaining that a matrix of datatype ```torch.int64``` (a so-called ```LongTensor```) can't be inverted, because there's no integer matrix ```M^-1``` by which we can multiply it such that ```M * M^-1 = I```. So we need to tell it explicitly that we want to treat matrix ```M``` as a floating-point matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3714, -0.2571,  0.0286],\n",
      "        [-0.0286, -0.0571,  0.2286],\n",
      "        [-0.0571,  0.2190, -0.2095]])\n",
      "tensor([[ 1.0000e+00,  8.9407e-08, -8.9407e-08],\n",
      "        [-3.3528e-08,  1.0000e+00, -1.4901e-08],\n",
      "        [-2.6077e-08, -1.4901e-08,  1.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "M2 = M2.float()\n",
    "M2inv = torch.inverse(M2)\n",
    "print(M2inv)\n",
    "I = M2.mm(M2inv)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** How do we convert this back to a LongTensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "I = I.round().long()\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```reshape``` is a kind of magical operation that repacks the elements of a matrix into a different shape by copying the elements row by row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(M)\n",
    "print(M.reshape((2, 6)))\n",
    "print(M.reshape(12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```torch``` supports the notion of **broadcasting**, which allows us to conveniently use elementwise operators over tensors of different orders. For instance, if we add a vector ```v``` of shape (3) to a matrix ```M``` of shape (4,3), then ```torch``` assumes that we want to add two (4,3) matrices. One is just ```M```. The other is 4 copies of vector ```v``` piled on top of each other to make another (4,3) matrix. Surprisingly this turns out to be useful. In general, you can use broadcasting to apply elementwise operations if one tensor has shape (a, b, ... n, n + 1, ... m) and the other has shape (n, n + 1, ... m). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = tensor([1, 2, 3])\n",
    "v_order2 = tensor([[1, 2, 3], [1, 2, 3], [1, 2, 3], [1, 2, 3]])\n",
    "M = tensor([[4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15]])\n",
    "print(M+v)\n",
    "print(M+v_order2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you think of an example of an order-3 tensor, famous in pop culture? Hint, its shape is (3, 3, 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![order-three tensor](img/mystery.jpg \"Order 3 tensor\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful order-3 tensor is an image file. Consider this image of the Swiss flag:\n",
    "\n",
    "![swiss flag](img/swiss.gif \"Swiss flag\")\n",
    "\n",
    "We can view the pixels as a 5x5 matrix. If this were a black-and-white image, it would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWISS_FLAG_BW = tensor([[0,0,0,0,0],[0,0,1,0,0],[0,1,1,1,0],[0,0,1,0,0],[0,0,0,0,0]])\n",
    "print(SWISS_FLAG_BW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it isn't a black-and-white image, it's a color image, which means that for each pixel, there's a red value (from 0-255), a green value (from 0-255), and a blue value (from 0-255)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Represent the Swiss flag as an order-three tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 5 at dim 1 (got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f5473b2b0000>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSWISS_FLAG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO: complete this line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSWISS_FLAG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The Swiss flag has shape: {}.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSWISS_FLAG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 5 at dim 1 (got 3)"
     ]
    }
   ],
   "source": [
    "SWISS_FLAG = pass # TODO: complete this line\n",
    "print(SWISS_FLAG)\n",
    "print('The Swiss flag has shape: {}.'.format(SWISS_FLAG.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it can be easier to use torch's tensor concatenation operations to construct these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255],\n",
      "         [255, 255, 255, 255, 255]],\n",
      "\n",
      "        [[  0,   0,   0,   0,   0],\n",
      "         [  0,   0, 255,   0,   0],\n",
      "         [  0, 255, 255, 255,   0],\n",
      "         [  0,   0, 255,   0,   0],\n",
      "         [  0,   0,   0,   0,   0]],\n",
      "\n",
      "        [[  0,   0,   0,   0,   0],\n",
      "         [  0,   0, 255,   0,   0],\n",
      "         [  0, 255, 255, 255,   0],\n",
      "         [  0,   0, 255,   0,   0],\n",
      "         [  0,   0,   0,   0,   0]]])\n",
      "The Swiss flag has shape: torch.Size([3, 5, 5]).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "SWISS_FLAG_RED = tensor([[255,255,255,255,255],\n",
    "                           [255,255,255,255,255],\n",
    "                           [255,255,255,255,255],\n",
    "                           [255,255,255,255,255],\n",
    "                           [255,255,255,255,255]])\n",
    "\n",
    "SWISS_FLAG_GREEN = tensor([[0,0,0,0,0],\n",
    "                           [0,0,255,0,0],\n",
    "                           [0,255,255,255,0],\n",
    "                           [0,0,255,0,0],\n",
    "                           [0,0,0,0,0]])\n",
    "\n",
    "SWISS_FLAG_BLUE  = tensor([[0,0,0,0,0],\n",
    "                           [0,0,255,0,0],\n",
    "                           [0,255,255,255,0],\n",
    "                           [0,0,255,0,0],\n",
    "                           [0,0,0,0,0]])\n",
    "\n",
    "SWISS_FLAG = torch.stack([SWISS_FLAG_RED, SWISS_FLAG_GREEN, SWISS_FLAG_BLUE])\n",
    "print(SWISS_FLAG)\n",
    "print('The Swiss flag has shape: {}.'.format(SWISS_FLAG.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually display this image in this notebook using a plotting library called ```matplotlib``` (which, if you don't have, make sure you ```pip install matplotlib```). The problem is, it expects the image tensor to have shape ```(5,5,3)```, rather than ```(3,5,5)```. In other words, rather than ```(color,x,y)``` coordinates, it expects ```(x,y,color)``` coordinates. Luckily, higher-order tensors have generalized transpose operations that swap arbitrary axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2])\n",
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "tensor([[[1, 2],\n",
      "         [5, 6]],\n",
      "\n",
      "        [[3, 4],\n",
      "         [7, 8]]])\n"
     ]
    }
   ],
   "source": [
    "T = tensor([[[1,2], [3,4]], [[5,6], [7,8]]])\n",
    "print(T.shape)\n",
    "print(T)\n",
    "print(T.transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "tensor([[[1, 5],\n",
      "         [3, 7]],\n",
      "\n",
      "        [[2, 6],\n",
      "         [4, 8]]])\n"
     ]
    }
   ],
   "source": [
    "print(T)\n",
    "print(T.transpose(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2],\n",
      "         [3, 4]],\n",
      "\n",
      "        [[5, 6],\n",
      "         [7, 8]]])\n",
      "tensor([[[1, 3],\n",
      "         [2, 4]],\n",
      "\n",
      "        [[5, 7],\n",
      "         [6, 8]]])\n"
     ]
    }
   ],
   "source": [
    "print(T)\n",
    "print(T.transpose(1,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Convert the Swiss flag into an image tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original flag shape: torch.Size([5, 5, 3]).\n",
      "The modified flag shape: torch.Size([5, 5, 3]).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x129de7490>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "new_flag = SWISS_FLAG.transpose(0,2)\n",
    "SWISS_FLAG = new_flag.transpose(0,1) # TODO: complete this line!\n",
    "print('The original flag shape: {}.'.format(SWISS_FLAG.shape))\n",
    "print('The modified flag shape: {}.'.format(new_flag.shape))\n",
    "imshow(new_flag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This tutorial is now complete. Please proceed to do the exercise \"Rubik\" from the Github Classroom:**\n",
    "\n",
    "https://classroom.github.com/classrooms/46545598-csci-378-spring-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
